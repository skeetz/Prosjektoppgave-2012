\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\@writefile{@@@}{\chapterbegin }
\@writefile{@@@}{\chapterbegin }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Introduction}{4}{chapter.1}}
\@writefile{lot}{\contentsline {xchapter}{Introduction}{4}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Formal Problem Statement}{5}{section.1.1}}
\newlabel{mixing_model}{{1.1}{5}{Formal Problem Statement\relax }{equation.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Single Sensor Blind Source Separation}{5}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}A Linear Mixing Model}{6}{subsection.1.1.2}}
\newlabel{linear_mixing_model}{{1.2}{6}{A Linear Mixing Model\relax }{equation.1.1.2}{}}
\newlabel{linear_unmixing_model}{{1.3}{6}{A Linear Mixing Model\relax }{equation.1.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Overview}{6}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{8}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Literature Review}{8}{chapter.2}}
\@writefile{lot}{\contentsline {xchapter}{Literature Review}{8}{chapter.2}}
\citation{bellSejnowski95}
\citation{hyvarinen2001}
\citation{roweisOneMic}
\citation{VargaHMMDecomp}
\citation{comon94}
\citation{fastICA}
\citation{davies2007}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Stationary linear mixing process and separation.\relax }}{9}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{pca_time_series}{{2.1}{9}{Stationary linear mixing process and separation.\relax \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Literature Review Process}{9}{section.2.1}}
\newlabel{reviewProcess}{{2.1}{9}{Literature Review Process\relax }{section.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Magnitude of hits on the most general terms\relax }}{9}{table.caption.4}}
\newlabel{tab:myfirsttable}{{2.1}{9}{Magnitude of hits on the most general terms\relax \relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Literature Overview}{10}{section.2.2}}
\newlabel{overview}{{2.2}{10}{Literature Overview\relax }{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Independent Component Analysis}{10}{subsection.2.2.1}}
\citation{comon94}
\citation{comon94}
\citation{bellSejnowski95}
\citation{pearlmutterParra}
\citation{fastICA}
\citation{hyvarinen2001}
\citation{mijovic2010}
\citation{bach}
\citation{VargaHMMDecomp}
\citation{roweisOneMic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hidden Markov Model Decomposition of Speech and Noise}{12}{subsection.2.2.2}}
\newlabel{vargasEqn1}{{2.2}{12}{Hidden Markov Model Decomposition of Speech and Noise\relax }{equation.2.2.2}{}}
\newlabel{vargasEqn2}{{2.3}{12}{Hidden Markov Model Decomposition of Speech and Noise\relax }{equation.2.2.3}{}}
\newlabel{vargasEqn3}{{2.4}{12}{Hidden Markov Model Decomposition of Speech and Noise\relax }{equation.2.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Factoral Hidden Markov Models}{13}{subsection.2.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Hidden Markov Model\relax }}{13}{figure.caption.6}}
\newlabel{hmm_figure}{{2.2}{13}{Hidden Markov Model\relax \relax }{figure.caption.6}{}}
\newlabel{roweisEqn1}{{2.5}{13}{Factoral Hidden Markov Models\relax }{equation.2.2.5}{}}
\newlabel{roweisEqn2}{{2.6}{13}{Factoral Hidden Markov Models\relax }{equation.2.2.6}{}}
\newlabel{roweisEqn3}{{2.7}{13}{Factoral Hidden Markov Models\relax }{equation.2.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Factorial Hidden Markov Model\relax }}{14}{figure.caption.7}}
\newlabel{fhmm_figure}{{2.3}{14}{Factorial Hidden Markov Model\relax \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Conclusion}{14}{section.2.3}}
\newlabel{conclusion}{{2.3}{14}{Conclusion\relax }{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Research Agenda}{14}{section.2.4}}
\newlabel{protocol}{{2.4}{14}{Research Agenda\relax }{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Background}{14}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Research Questions}{15}{subsection.2.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Search Strategy}{15}{subsection.2.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Databases}{15}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{List of Search Terms}{15}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{Inclusion and Quality Criteria}{16}{section*.10}}
\citation{pearson1901}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Principal Component Analysis}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Principal Component Analysis}{17}{chapter.3}}
\@writefile{lot}{\contentsline {xchapter}{Principal Component Analysis}{17}{chapter.3}}
\newlabel{pca}{{3}{17}{Principal Component Analysis\relax }{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces After projecting the data onto the dark line, little variation left in the dataset.\relax }}{18}{figure.caption.11}}
\newlabel{pca1}{{3.1}{18}{After projecting the data onto the dark line, little variation left in the dataset.\relax \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Intuition}{18}{section.3.1}}
\newlabel{pca_intuition}{{3.1}{18}{Intuition\relax }{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Projecting the data onto the dark line we reduce dimensionality while keeping a large portion of the variation characterizing the data.\relax }}{19}{figure.caption.12}}
\newlabel{pca2}{{3.2}{19}{Projecting the data onto the dark line we reduce dimensionality while keeping a large portion of the variation characterizing the data.\relax \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Formal Statement}{19}{section.3.2}}
\newlabel{pca_lagrangian}{{3.1}{20}{Formal Statement\relax }{equation.3.2.1}{}}
\newlabel{pca_gradient}{{3.2}{20}{Formal Statement\relax }{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Singular Value Decomposition}{20}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}PCA Application to Blind Source Separation}{21}{section.3.3}}
\newlabel{pca_bss}{{3.3}{21}{PCA Application to Blind Source Separation\relax }{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces PCA Source Separation.\relax }}{22}{figure.caption.13}}
\newlabel{pca_time_series}{{3.3}{22}{PCA Source Separation.\relax \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Standardized data points vs eigenvectors.\relax }}{22}{figure.caption.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Standardized data projected onto eigenvectors.\relax }}{23}{figure.caption.15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Independent Component Analysis}{24}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Independent Component Analysis}{24}{chapter.4}}
\@writefile{lot}{\contentsline {xchapter}{Independent Component Analysis}{24}{chapter.4}}
\newlabel{ica}{{4}{24}{Independent Component Analysis\relax }{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Equivalent Statements of ICA}{24}{section.4.1}}
\newlabel{equiv_ica}{{4.1}{24}{Equivalent Statements of ICA\relax }{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Kurtosis Maximization}{25}{subsection.4.1.1}}
\newlabel{kurtosis_eqn}{{4.3}{25}{Kurtosis Maximization\relax }{equation.4.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Maximum Mutual Information}{25}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Limitations of the ICA Model}{25}{section.4.2}}
\newlabel{ICA_restrictions}{{4.2}{25}{Limitations of the ICA Model\relax }{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Maximum Likelihood ICA in the Linear Mixing Model}{26}{section.4.3}}
\newlabel{linear_model_ica}{{4.3}{26}{Maximum Likelihood ICA in the Linear Mixing Model\relax }{section.4.3}{}}
\newlabel{ica_prob}{{4.7}{26}{Maximum Likelihood ICA in the Linear Mixing Model\relax }{equation.4.3.7}{}}
\newlabel{max_likelihood}{{4.10}{27}{Maximum Likelihood ICA in the Linear Mixing Model\relax }{equation.4.3.10}{}}
\newlabel{bayes}{{4.11}{27}{Maximum Likelihood ICA in the Linear Mixing Model\relax }{equation.4.3.11}{}}
\newlabel{ica_loglikelihood}{{4.12}{27}{Maximum Likelihood ICA in the Linear Mixing Model\relax }{equation.4.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}A Stochastic Gradient Descent Rule for ICA}{27}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Preprocessing}{27}{subsection.4.3.2}}
\newlabel{ica_preprocessing}{{4.3.2}{27}{Preprocessing\relax }{subsection.4.3.2}{}}
\newlabel{ica_listing}{{\caption@xref {ica_listing}{ on input line 1067}}{28}{A Stochastic Gradient Descent Rule for ICA\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textsc  {Matlab} code for ML ICA by stochastic block gradient descent.\relax }}{28}{figure.caption.16}}
\newlabel{mlica_code}{{4.1}{28}{\textsc {Matlab} code for ML ICA by stochastic block gradient descent.\relax \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces ICA on a $2\times 2$ BSS problem. Note the ``sign reversal'' for the blue sine wave (cf. Section \ref  {ICA_restrictions}).\relax }}{28}{figure.caption.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Separating a speech signal (top left) from background music (top right) by ICA. Here we also observe that the sign of the original speech signal is reversed in the bottom right recovered signal. \relax }}{29}{figure.caption.18}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}BSS by ICA}{29}{section.4.4}}
\newlabel{BSS_ICA}{{4.4}{29}{BSS by ICA\relax }{section.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Limitations and Comparison with PCA}{29}{section.4.5}}
\newlabel{ica_conclusions}{{4.5}{29}{Limitations and Comparison with PCA\relax }{section.4.5}{}}
\citation{roweis}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Single Sensor Blind Source Separation}{30}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Single Sensor Blind Source Separation}{30}{chapter.5}}
\@writefile{lot}{\contentsline {xchapter}{Single Sensor Blind Source Separation}{30}{chapter.5}}
\newlabel{ssbss_chap}{{5}{30}{Single Sensor Blind Source Separation\relax }{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Time Frequency Signal Representation}{30}{section.5.1}}
\newlabel{timeFreqRep}{{5.1}{30}{Time Frequency Signal Representation\relax }{section.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Time domain representation of a male voice counting from one to ten.\relax }}{31}{figure.caption.19}}
\newlabel{stft}{{5.1}{31}{Time Frequency Signal Representation\relax }{equation.5.1.1}{}}
\newlabel{spectrogram}{{5.2}{32}{Time Frequency Signal Representation\relax }{equation.5.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Log Max Approximation}{32}{section.5.2}}
\newlabel{logmax_fig}{{\caption@xref {logmax_fig}{ on input line 1222}}{32}{Log Max Approximation\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Log-max approximation.\relax }}{32}{figure.caption.20}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Latent Variable BSS}{32}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Gaussian Mixture Models}{33}{subsection.5.3.1}}
\newlabel{gmm_general}{{5.3.1}{33}{Gaussian Mixture Models\relax }{subsection.5.3.1}{}}
\newlabel{gaussmix_eqn}{{5.3}{33}{Gaussian Mixture Models\relax }{equation.5.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Generative Model and Estimation}{33}{subsection.5.3.2}}
\newlabel{generative_model}{{5.3.2}{33}{Generative Model and Estimation\relax }{subsection.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A mixture of gaussians distribution with two components.\relax }}{34}{figure.caption.21}}
\newlabel{gmm_fig}{{5.3}{34}{A mixture of gaussians distribution with two components.\relax \relax }{figure.caption.21}{}}
\newlabel{maxvq_eqn}{{5.4}{34}{Generative Model and Estimation\relax }{equation.5.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}A Generic Inference Procedure}{34}{subsection.5.3.3}}
\newlabel{bayes_eqn1}{{5.5}{35}{A Generic Inference Procedure\relax }{equation.5.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}An Improved Pruning Method for MAXVQ Inference}{35}{subsection.5.3.4}}
\newlabel{bound}{{5.7}{36}{An Improved Pruning Method for MAXVQ Inference\relax }{equation.5.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Factorial Hidden Markov Model for BSS}{36}{section.5.4}}
\newlabel{fhmm}{{5.4}{36}{Factorial Hidden Markov Model for BSS\relax }{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Initialization}{36}{subsection.5.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Pseudocode for MAXVQ algorithm.\relax }}{37}{figure.caption.22}}
\newlabel{maxvq_pseudo}{{5.4}{37}{Pseudocode for MAXVQ algorithm.\relax \relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Spectrogram of male voice counting from one to ten.\relax }}{38}{figure.caption.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Separation}{38}{subsection.5.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Results}{39}{section.5.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{40}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Conclusion}{40}{chapter.6}}
\@writefile{lot}{\contentsline {xchapter}{Conclusion}{40}{chapter.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Mathematical Concepts}{41}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {xchapter}{Mathematical Concepts}{41}{appendix.A}}
\@writefile{lot}{\contentsline {xchapter}{Mathematical Concepts}{41}{appendix.A}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Linear Algebra}{41}{section.A.1}}
\@writefile{toc}{\contentsline {subsubsection}{The Eigenvector - Eigenvalue Problem}{41}{section*.24}}
\@writefile{toc}{\contentsline {subsubsection}{Singular Value Decomposition}{41}{section*.25}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Statistics and Optimization}{41}{section.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Maximum Likelihood Estimation}{42}{subsection.A.2.1}}
\newlabel{ml-estimation}{{A.2.1}{42}{Maximum Likelihood Estimation\relax }{subsection.A.2.1}{}}
\newlabel{ml-generic}{{A.1}{42}{Maximum Likelihood Estimation\relax }{equation.A.2.1}{}}
\newlabel{ml-iid}{{A.2}{42}{Maximum Likelihood Estimation\relax }{equation.A.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Mathematical Optimization}{42}{subsection.A.2.2}}
\newlabel{optimization}{{A.2.2}{42}{Mathematical Optimization\relax }{subsection.A.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Descent}{42}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Spectral Analysis}{42}{section.A.3}}
\bibcite{comon94}{1}
\bibcite{bellSejnowski95}{2}
\bibcite{pearlmutterParra}{3}
\bibcite{hyvarinen2001}{4}
\bibcite{bach}{5}
\bibcite{fastICA}{6}
\bibcite{roweisOneMic}{7}
\bibcite{davies2007}{8}
\bibcite{cardoso98}{9}
\bibcite{mijovic2010}{10}
\bibcite{VargaHMMDecomp}{11}
\bibcite{pearson1901}{12}
\@writefile{@@@}{\chapterbegin }
\bibcite{norvig_russel}{13}
\global\mtcsecondpartfalse
