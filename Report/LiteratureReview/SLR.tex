\documentclass[11pt, oneside, a4paper]{article}  

% Input and math
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}

% Hyperlinks
\usepackage{hyperref}

% Colors
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}


% Source code listings (see below begindoc), graphics
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}

\begin{document}

% For source code listings
\lstset{language=Matlab,
   keywords={break,case,catch,continue,else,elseif,end,for,function,
      global,if,otherwise,persistent,return,switch,try,while},
   basicstyle=\ttfamily,
   keywordstyle=\color{blue},
   commentstyle=\color{red},
   stringstyle=\color{dkgreen},
   numbers=left,
   numberstyle=\tiny\color{gray},
   stepnumber=1,
   numbersep=10pt,
   backgroundcolor=\color{white},
   tabsize=4,
   showspaces=false,
   showstringspaces=false}

\title{Blind Source Separation\\--\\Literature Search Protocol}
\author{Anders Røsæg Pedersen\\Ulf Nore}
\date{NTNU, Fall 2012}    % type date between braces
\maketitle

\begin{abstract}
Blabla..
\end{abstract}

\tableofcontents

\section{Introduction}

The blind source separation problem refers to the process of recovering one or more signals that have been mixed in some unknown manner and possibly also contamined by noise. Without any assumptions on the mixing process, this problem is ill-poised. In practice therefore, all BSS methods rely on some stylized fact about the nature of the signals and/or the mixing process. It is therefore useful to dichotomize BSS methods by these assumptions.

Arguably, two of the most important facts characterizing a mixing process, are its temporal dynamics and the number of degrees of freedom. The first point refers to whether the nature of the mixing process changes over time, that is if the mixing matrix at time $t+k$ is different from that at time $t$ for $k>0$. The number of degrees of freedom is the same concept as in linear algebra - the connection is apparent by seeing the mixing process as a system of linear equations. If $m$ is the number of observed signals and $n$ the number of sources, the system is said to be \emph{underdetermined} if $m<n$ and conversely \emph{overdetermined} if $m>n$. 

We can also differentiate between method based on the nature of input data. Early BSS research often considered the case of $n=m$, which allows one to work with data in the time domain. For undetermined systems, it is commonplace to work with some transformation of the data, which in the case of audio data a time-frequency representation. Common methods include the \emph{short-term Fourier transform} and the \emph{wavelet transform}.

\section{Literature Review Process} % Anders



\section{Literature Overview}


\subsection{Independent Component Analysis} %% Ulf

%% Standard ICA
Among the most common approaches to blind source separation is
independent component analysis (ICA). Common definitions of ICA use
either the maximization of independence or minimization of mutual information between the source
signals\footnote{It should be noted that while this text presents ICA
  in terms of blind source separation, the method is applicable to a
  wide array of machine learning problems including dimension
  reduction, classification, and de-noising.}. 

The classical reference on ICA
is \ref{comon94}, where the method of minimization of mutual
information between sources is presented. \ref{comon94} also presents an
analysis of the ambiguities and limitations of ICA, hereunder the permutation of
sources, scaling and non-gaussianity. 

Computationally, ICA can be stated in many different forms,
hereunder neural networks (Bell and Sejnowski \ref{bellSejnowski95}),
maximum likelihood (\ref{pearlmutterParra}) and maximization of
non-gausianity as measured by excess kurtosis, which is used in the
popular FastICA algorithm (\ref{fastICA}).

For a thorough survey on the classical literature on ICA, see \ref{hyvarinen2001}.


%% Extensions to Single Channel Mix



\subsection{Factoral Hidden Markov Models} %% Anders


\subsection{Wavelets} %Ulf




Ichir Hidden markov models

\section{Conclusion}

\theappendix

\section{Protocol}


\begin{thebibliography}{99}

\bibitem{comon94} Comon, P. (1994). 
``Independent Component Analysis: a new concept?'', 
Signal Processing, 36(3):287–314

\bibitem{bellSejnowski95} Bell, A.J. and Sejnowski, T.J. (1995). 
``An information maximization approach to blind separation and blind deconvolution'', 
Neural Computation, 7, 1129-1159

\bibitem{pearlmutterParra}Pearlmutter, B. A. and Parra, L. C.(YYYY),
``A Context-Sensitive Generalization of ICA''. 

\bibitem{hyvarinen2001}Hyvarinen A (2001).
``Independent Component Analysis'',
Neural Computing Surveys, Neural Computing Surveys, vol 2.

  
\end{thebibliography}




\end{document}
