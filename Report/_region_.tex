\message{ !name(main.tex)}\documentclass[]{report}   % list options between brackets
\usepackage{amsmath}              % list packages between braces

% type user-defined commands here

\begin{document}

\message{ !name(main.tex) !offset(-3) }


\title{Blind Source Separation}
\author{}
\date{}    % type date between braces
\maketitle

\begin{abstract}

\end{abstract}


\chapter{Introduction}

\section{Formal Problem Statement}

We now provide a notation leading to a mathematical statement of the blind source separation (BSS) problem. We let $S(t)\in \mathbf{R}^n$ for $t>0, n>0$ denote the signals generated by $n$ sources. Similarly, we let $X(t)\in \mathbf{R}^m$ for $t>0, n>0$ the observed sensor readings resulting from the emitted signals. A \emph{mixing model} $f(S,t)$ defines the relationship between source and observed signal:

\begin{equation}\label{mixing_model}
  X = f(S,t)
\end{equation}

As only the observed value $X$ is known, we need to determine the inverse $f^{-1}(S,t)$, that is, the \emph{unmixing model}.

\subsection{Single Microphone Blind Source Separation}

A particular instance of the BSS problem, is the single microphone\footnote{The reader may subsitute ``microphone'' for whatever recording instrument he may choose.} blind source separation (SMBSS) problem, to which we will devote particular attention. In the SBMSS problem, we have one or more source signals, but the observed signal $X(t)$ is a scalar. This introduces problems as this instance does not lend itself to solutions by means of the ``standard'' methods we consider in the standard BSS problem.

\subsection{A Simple Linear Mixing Model}

The simplest mixing model is a noiseless, stationary linear mixing model. The stationarity assumption means that the mixing model does not change as a function of time, so the $t$ argument in Equation \ref{mixing_model} can be omitted. With $T$ measurements, $N$ sources, and $M$ sensors, this model can be defined as:


\begin{equation}\label{linear_mixing_model}
  X = AS
\end{equation}

With $X \in \mathbf{R}^{N\times T}$, $A \in \mathbf{R}^{N\times M}$ and $S \in \mathbf{R}^{M\times T}$. The problem of determining the unmixing model now consists of computing the inverse $W = A^{-1}$, so that the original signal 

\begin{equation}\label{linear_unmixing_model}
S = WX
\end{equation}

can be recovered.

From Equation \ref{linear_mixing_model}, we can see that the blind source separation problem, even in this simplest case, is ill-poised, that is we are trying to determine both $A$ and $S$ given only $X$. This implies that we need to impose certain assumptions on the nature of the data. As will be made apparent later, which assumptions are made, gives rise to different solution approaches. For the purpose of this study, we will be quite restrictive in terms of the assumptions we make, hence the term \emph{blind} source separation. The type of assumptions made are primarily related to statistical properties of the sources, such as for instance statistically independence, an assumption that leads to the ICA solution.

\chapter{Principal Component Analysis}

Principal component analysis (PCA) is a technique that uses orthogonal projection to represent data in a lower dimensional space. This is useful in several applications, hereunder visualization and detection of so-called \emph{latent variables}. The principal components (PCs) are the basis of the subspace onto which the data is projected, and are such that the variance explained by each component is maximized; that is, the first PC explains a higher proportion of variance than the second PC and so forth. We can therefore, by retaining only the first few components acheive a representation of the data containing the most of the variance exhibited.



\section{Non-Linear PCA}


\chapter{Independent Component Analysis}

\section{Restrictions in the ICA Model}\label{ICA_restrictions}

\section{ICA in the Linear Mixing Model}

In this section we will show how the ICA assumptions can be applied to the linear mixing model. In this context we will give a maximum likelihood estimator for the unmixing matrix $W$, and an algorithm to solve this model by gradient descent.

Let $p_s(s_i)$ be the probability distribution for source $i$, then, assuming the sources are independent the joint distribution of all the $n$ sources is given by the product of the marginals:

\begin{equation}
  p(s) = \Pi_{i=1}^n p_s(s_i)
\end{equation}

We now substitute in the unmixing model (Equation \ref{linear_unmixing_model}) and obtain:

\begin{equation}
  p(s) = \Pi_{i=1}^n p_s(WX) \det W
\end{equation}

The unmixing matrix is the target parameter of our maximum likelihood approach. That is, we seek set the coefficients of the unmixing matrix so as to maximize the likelihood of observing the actual data. If our dataset consists of $T$ observations $X = \{x_1,x_2,...,x_T\}$, the log-likehliood function is:

\begin{equation}
  l(W) =\log Prob(X|W)= \Sigma_{t=1}^T \log p_s(WX)+\log \det W
\end{equation}

\chapter{Results}

\chapter{Discussion}

\begin{thebibliography}{9}
  
\end{thebibliography}

\end{document}

\message{ !name(main.tex) !offset(-101) }
