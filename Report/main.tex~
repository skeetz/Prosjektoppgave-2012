\documentclass[11pt, oneside, a4paper]{report}  

% Input and math
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}

% Hyperlinks
\usepackage{hyperref}

% Colors
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}


% Source code listings (see below begindoc), graphics
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}

\begin{document}

% For source code listings
\lstset{language=Matlab,
   keywords={break,case,catch,continue,else,elseif,end,for,function,
      global,if,otherwise,persistent,return,switch,try,while},
   basicstyle=\ttfamily,
   keywordstyle=\color{blue},
   commentstyle=\color{red},
   stringstyle=\color{dkgreen},
   numbers=left,
   numberstyle=\tiny\color{gray},
   stepnumber=1,
   numbersep=10pt,
   backgroundcolor=\color{white},
   tabsize=4,
   showspaces=false,
   showstringspaces=false}

\title{Blind Source Separation}
\author{}
\date{}    % type date between braces
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\chapter{Introduction}

Blabla... gpp

\section{Formal Problem Statement}

We now provide a notation leading to a mathematical statement of the blind source separation (BSS) problem. We let $S(t)\in \mathbf{R}^n$ for $t>0, n>0$ denote the signals generated by $n$ sources. Similarly, we let $X(t)\in \mathbf{R}^m$ for $t>0, n>0$ the observed sensor readings resulting from the emitted signals. A \emph{mixing model} $f(S,t)$ defines the relationship between source and observed signal:

\begin{equation}\label{mixing_model}
  X = f(S,t)
\end{equation}

As only the observed value $X$ is known, we need to determine the inverse $f^{-1}(S,t)$, that is, the \emph{unmixing model}.

\subsection{Single Sensor Blind Source Separation}

A particular instance of the BSS problem, is the single sensor blind source separation (SSBSS) problem, to which we will devote particular attention. In the SSMSS problem, we have one or more source signals, but the observed signal $X(t)$ is a scalar. This introduces problems as this instance does not lend itself to solutions by means of the ``standard'' methods we consider in the standard BSS problem. Chapter \ref{ssbss_chap} is devoted to the SSBSS problem.

\subsection{A Linear Mixing Model}

The simplest mixing model is a noiseless, stationary linear mixing model. The stationarity assumption means that the mixing model does not change as a function of time, so the $t$ argument in Equation \ref{mixing_model} can be omitted. With $T$ measurements, $N$ sources, and $M$ sensors, this model can be defined as:


\begin{equation}\label{linear_mixing_model}
  X = AS
\end{equation}

With $X \in \mathbf{R}^{N\times T}$, $A \in \mathbf{R}^{N\times M}$
and $S \in \mathbf{R}^{M\times T}$. The problem of determining the
unmixing model now consists of computing the inverse $W = A^{-1}$, so
that the original signal:

\begin{equation}\label{linear_unmixing_model}
S = WX
\end{equation}

can be recovered. This is to say that the estimate of the original
signal $j$ at time $t$ is computed as the $j$th row of $W$ times the
$t$th column of $X$.

From Equation \ref{linear_mixing_model}, we can see that the blind source separation problem, even in this simplest case, is ill-poised, that is we are trying to determine both $A$ and $S$ given only $X$. This implies that we need to impose certain assumptions on the nature of the data. As will be made apparent later, which assumptions are made, gives rise to different solution approaches. For the purpose of this study, we will be quite restrictive in terms of the assumptions we make, hence the term \emph{blind} source separation. The type of assumptions made are primarily related to statistical properties of the sources, such as for instance statistically independence, an assumption that leads to the ICA solution.

\section{Overview}

In the next chapters we will be looking at a few different algorithms for solving various instances of the BSS problem. Each algorithm has its own merits depending to a large extent on the assumptions we make about the data. An overview of these follow in the Table \ref{overviewTable}.

\begin{center}
  \begin{table}[h!]
    \begin{tabular}{ | l | l | l | p{5cm} |}
      \hline
      \textbf{Method} & \textbf{Data Characteristic} & \textbf{Description} \\ \hline
      PCA & Blabla & Blablabla \\ 
      \hline
    \end{tabular}
    \caption{Overview over different approaches to blind source separation.}
    \label{overviewTable}
  \end{table}
\end{center}

\chapter{Principal Component Analysis}

Principal component analysis (PCA) is a eigenvector-based,
non-probabilistic technique that uses orthogonal projection to
represent data in a lower dimensional subspace spanned by the $k$
first eigenvectors of the covariance matrix. The eigenvectors form an
orthogonal basis for the data such that a projection onto the
eigenvectors will decorrelate the data. In the next section we will
derive this result by maximizing the variance of an axis of projection.

PCA is useful in several applications, hereunder visualization and
detection of so-called \emph{latent variables}. The principal
components (PCs) are the basis of the subspace onto which the data is
projected, and are such that the variance explained by each component
is maximized; that is, the first PC explains a higher proportion of
variance than the second PC and so forth. We can therefore, by
retaining only the first few components acheive a representation of
the data containing the most of the variance exhibited by the
assumption that the PCs accounting for the smallest portion of
variance are noise.


The next section presents PCA from two different but equivalent
perspectives; first solving for the direction of maximal variation
using the method of Lagrange multipliers, and subsequently by singular
value decomposition which. The latter is the more computationally
efficient, and the rationale for this approach is easy to see once the
first perspective is known. We then proceed to looking at how PCA can
be applied to the blind source problem, before we finally look at a
non-linear extension of PCA.


\section{Formal Statement}

Let $x_i \in \mathbf{R}^n$ denote the $i$'th observation of a dataset
of $m$ observations. We now want to project our data onto a vector $u$
in $\mathbf{R}^n$ so as to maximize the variance of the resulting
projection $\sum_{i=1}^m x_i^Tu$ subject to the constraint
$|u|=1$. Under the assumption that $X$ is standardized to zero mean
and unit variance, the Lagrangian is then given by Equation \ref{pca_lagrangian}:

  \begin{equation}
    \label{pca_lagrangian}
    \begin{array}{lcl}
      \mathcal{L}(u,\lambda) & = & \frac{1}{m} \sum_{i=1}^m (x_i^Tu)^2 - \lambda (u^Tu -1) \\
      \\& = & \frac{1}{m} \sum_{i=1}^m (u^Tx_i)^T(x_i^Tu) - \lambda (u^Tu -1) \\
      \\& = & \frac{1}{m} \sum_{i=1}^m u^T(x_ix_i^T)u - \lambda (u^Tu -1) \\
      \\& = & \frac{1}{m}  u^T\sum_{i=1}^m(x_ix_i^T)u - \lambda (u^Tu -1) \\
      \\& = & \frac{1}{m} u^T\Sigma u - \lambda (u^Tu -1) \\
    \end{array}
  \end{equation}

Here, $\Sigma = \sum_{i = 1}^m x_ix_i^T$ is the covariance matrix. Setting the gradient equal to zero yields Equation \ref{pca_gradient}:

\begin{equation}
  \label{pca_gradient}
  \nabla_u \mathcal{L}(u, \lambda) = \Sigma u - \lambda u = 0
\end{equation}

Equation \ref{pca_gradient} shows that the direction of maximum variance $u$, which we will refer to as the first principal component, is the first eigenvector of the covariance matrix of the dataset. By similar means it can be shown that the second eigenvector points in the direction of largest variance \emph{orthogonal} to the first eigenvector and so forth.

\subsection{Singular Value Decomposition}

For a high dimensional dataset (e.g. $n = 10,000$), which is
frequently the case working with for instance image or video data, the covariance matrix will have $10,000\times 10,000 = 100,000,000$ entries, which is computationally untractable. Hence, PCA is usually implemented in terms of \emph{singular value decomposition} (SVD). For an $m\times n$ matrix $X$, the SVD is a factorization such that:

\begin{equation}
  X = US V^{T}
\end{equation}

Here, $U \in \mathbf{R}^{m\times m}$, $S \in \mathbf{R}^{m\times n}$,
and $U \in \mathbf{R}^{n\times n}$. The SVD relates to the eigenvalue
problem (Equation \ref{pca_gradient}) as follows:

\begin{itemize}
\item The columns of $U$ form the projections of $X$ onto the
  eigenvectors $V$.
  \item The entries $s_{ii}$ on the leading diagonal of $S$ are the
    eigenvalues of $\Sigma = X^TX$.
  \item The top $k$ columns of $V$ are the top $k$ eigenvectors of
    $\Sigma = X^T X$
\end{itemize}

In \textsc{Matlab}, we can perform SVD by a single line of code
(subsequent to standardizing the data to zero mean and unit variance):

\begin{figure}[!htpb]
  \begin{lstlisting}[frame=single]
   [U,S,V] = svd(X' * X);
  \end{lstlisting}
  \caption{\textsc{Matlab} code for SVD.}
  \label{svd_code}
\end{figure}

We will not go into the derivation of this result as SCD is covered
in most textbooks on linear algebra or basic numerical
mathematics. Rather, we will proceed to show how PCA can be applied to
BSS, and what assumptions it requires us to make about the data.



\section{Application to Blind Source Separation}

Blabla..


\begin{figure}
  \centering
  \hrule
  \includegraphics[width = .9\textwidth]{Figures/pca_signal_time_series}
  \hrule
  \caption{PCA Source Separation.}
\end{figure}

\begin{figure}
  \centering
  \hrule
  \includegraphics[width = .9\textwidth]{Figures/pca_data_eigs}
  \hrule
  \caption{Standardized data points vs eigenvectors.}
\end{figure}

\begin{figure}
  \centering
  \hrule
  \includegraphics[width = .9\textwidth]{Figures/pca_eig_projection}
  \hrule
  \caption{Standardized data projected onto eigenvectors.}
\end{figure}

\chapter{Independent Component Analysis}

We have seen that PCA finds the basis of a subspace in which the variance is
maximized in the direction of the basis vectors and the covariance
between the data is zero. ICA on the other hand, seeks to find basis
vectors that are statistically independent. This is a stronger
property than simply being uncorrelated as independence implies
uncorrelatedness, while the opposite is not true. 

ICA in contrast to PCA does not have analytic solutions in the general case, 
so a numerical optimization method is usually applied in computing the ICA transform.

\section{Limitations of the ICA Model}\label{ICA_restrictions}

Blabla


\begin{itemize}
  \item Non-Gaussian..
  \item Ordering of signals..
  \item ``Sign reversal'' (rotational invariance??)..
  \item Blabla..
\end{itemize}


\section{ICA in the Linear Mixing Model}

\subsection{Equivalent Specifications of ICA}

ICA can be derived by several different approaches:
\begin{itemize}
  \item Maximum likelihood
  \item Kurtosis maximization
  \item Maximum differential entropy
  \item Blabla..
\end{itemize}



\subsection{Maximum Likelihood Derivation}\label{ml_ica}

Let $p_s(s_i)$ be the probability density function for source $i$, then, assuming the sources are independent the joint distribution of all the $n$ sources is given by the product of the marginals:

\begin{equation}
  p(s) = \Pi_{i=1}^n p_s(s_i)
\end{equation}

We now substitute in the unmixing model (Equation \ref{linear_unmixing_model}) and obtain:

\begin{equation}
  p(s) = \Pi_{i=1}^n p_s(WX) \cdot |W|
\end{equation}

The unmixing matrix is the target parameter of our maximum likelihood approach. That is, we seek set the coefficients of the unmixing matrix so as to maximize the likelihood of observing the actual data. If our dataset consists of $T$ observations $X = \{x_1,x_2,...,x_T\}$, the log-likehliood function is:

\begin{equation}
  l(W) =\log Prob(X|W)= \Sigma_{t=1}^T \log p_s(WX)+\log |W|
\end{equation}

As the ICA is incompatible with a Gaussian source distribution, a common choice for specifying $P_s$ is either the sigmoid $p_s(s) = \frac{1}{1+e^{-s}}$, or the laplacian $p_s(s) = \frac{1}{2}e^{-|s|}$.

\begin{figure}[!htpb]
  \begin{lstlisting}[frame=single]
    code here...?
  \end{lstlisting}
  \caption{\textsc{Matlab} code for ML ICA by gradient descent.}
  \label{mlica_code}
\end{figure}


\subsection{FastICA}\label{fastICA}

\begin{figure}[!htpb]
  \begin{lstlisting}[frame=single]
    code here...?
  \end{lstlisting}
  \caption{\textsc{Matlab} code for FastICA algorithm.}
  \label{mlica_code}
\end{figure}


\subsection{Preprocessing}\label{ica_preprocessing}

Whitening transform...

\section{BSS by ICA}\label{BSS_ICA}


\begin{figure}
  \centering
  \hrule
  \includegraphics[width = .9\textwidth]{Figures/ica_simple}
  \hrule
  \caption{ICA on a $2\times 2$ BSS problem. Note the ``sign reversal'' for the blue sine wave (cf. Section \ref{ICA_restrictions}).}
\end{figure}

\begin{figure}
  \centering
  \hrule
  \includegraphics[width = .9\textwidth]{Figures/ica_music_speech}
  \hrule
  \caption{Separating a speech signal (top left) from background music (top right) by ICA. Here we also observe that the sign of the original speech signal is reversed in the bottom right recovered signal. }
\end{figure}




\section{Limitations and Comparison with PCA}\label{ica_conclusions}




\chapter{Single Sensor Blind Source Separation}\label{ssbss_chap}

todo.

\chapter{Conclusion}

\begin{thebibliography}{9}
  
\end{thebibliography}

\end{document}
